{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqpUaNkl1WNSlwoeeewueI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/map72ru/python_data_lib/blob/main/Lesson3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5eKmFoDQz4p"
      },
      "source": [
        "# Тема “Обучение с учителем”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV0So5rWQ648"
      },
      "source": [
        "#### Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-A3eXL_Q9j0"
      },
      "source": [
        "Импортируйте библиотеки pandas и numpy.\n",
        "Загрузите \"Boston House Prices dataset\" из встроенных наборов данных библиотеки sklearn. Создайте датафреймы X и y из этих данных.\n",
        "Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки\n",
        "составлял 30% от всех данных, при этом аргумент random_state должен быть равен 42.\n",
        "Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model.\n",
        "Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых.\n",
        "Вычислите R2 полученных предказаний с помощью r2_score из модуля sklearn.metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-vXbNoPQ3DM",
        "outputId": "65bfc0b1-8e0a-4263-debe-c1b852cd2a44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "ds = load_boston()\n",
        "x = pd.DataFrame(ds.data)\n",
        "y = pd.DataFrame(ds.target)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42) \n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lk = LinearRegression()\n",
        "lk.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lk.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "r2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7112260057484974"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdXEl2EncXHF"
      },
      "source": [
        "#### Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONj7HdwEckVf"
      },
      "source": [
        "Создайте модель под названием model с помощью RandomForestRegressor из модуля sklearn.ensemble.\n",
        "Сделайте агрумент n_estimators равным 1000,\n",
        "max_depth должен быть равен 12 и random_state сделайте равным 42.\n",
        "Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression,\n",
        "но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0],\n",
        "чтобы получить из датафрейма одномерный массив Numpy,\n",
        "так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно применение массивов вместо датафрейма.\n",
        "Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из предыдущего задания.\n",
        "Напишите в комментариях к коду, какая модель в данном случае работает лучше.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHXKGZZYcpif",
        "outputId": "7ccdc2b4-6d6f-4869-fab5-f00e34b59341",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfr = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)\n",
        "\n",
        "rfr.fit(x_train, y_train.values[:, 0])\n",
        "y_pred = rfr.predict(x_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "r2\n",
        "# Модель случайный лес в данном случае работает лучше"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8749965273218174"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-U_dE4yfn16"
      },
      "source": [
        "#### Задание 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OouNVK2kf0rU"
      },
      "source": [
        "Вызовите документацию для класса RandomForestRegressor,\n",
        "найдите информацию об атрибуте feature_importances_.\n",
        "С помощью этого атрибута найдите сумму всех показателей важности,\n",
        "установите, какие два признака показывают наибольшую важность.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxh9WP9fgqy5",
        "outputId": "5c4ec090-c9b3-4a57-95a2-2e33c1caf140",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "help(RandomForestRegressor)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class RandomForestRegressor in module sklearn.ensemble._forest:\n",
            "\n",
            "class RandomForestRegressor(ForestRegressor)\n",
            " |  A random forest regressor.\n",
            " |  \n",
            " |  A random forest is a meta estimator that fits a number of classifying\n",
            " |  decision trees on various sub-samples of the dataset and uses averaging\n",
            " |  to improve the predictive accuracy and control over-fitting.\n",
            " |  The sub-sample size is always the same as the original\n",
            " |  input sample size but the samples are drawn with replacement if\n",
            " |  `bootstrap=True` (default).\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <forest>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  n_estimators : integer, optional (default=10)\n",
            " |      The number of trees in the forest.\n",
            " |  \n",
            " |      .. versionchanged:: 0.22\n",
            " |         The default value of ``n_estimators`` changed from 10 to 100\n",
            " |         in 0.22.\n",
            " |  \n",
            " |  criterion : string, optional (default=\"mse\")\n",
            " |      The function to measure the quality of a split. Supported criteria\n",
            " |      are \"mse\" for the mean squared error, which is equal to variance\n",
            " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
            " |      absolute error.\n",
            " |  \n",
            " |      .. versionadded:: 0.18\n",
            " |         Mean Absolute Error (MAE) criterion.\n",
            " |  \n",
            " |  max_depth : integer or None, optional (default=None)\n",
            " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
            " |      all leaves are pure or until all leaves contain less than\n",
            " |      min_samples_split samples.\n",
            " |  \n",
            " |  min_samples_split : int, float, optional (default=2)\n",
            " |      The minimum number of samples required to split an internal node:\n",
            " |  \n",
            " |      - If int, then consider `min_samples_split` as the minimum number.\n",
            " |      - If float, then `min_samples_split` is a fraction and\n",
            " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
            " |        number of samples for each split.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_samples_leaf : int, float, optional (default=1)\n",
            " |      The minimum number of samples required to be at a leaf node.\n",
            " |      A split point at any depth will only be considered if it leaves at\n",
            " |      least ``min_samples_leaf`` training samples in each of the left and\n",
            " |      right branches.  This may have the effect of smoothing the model,\n",
            " |      especially in regression.\n",
            " |  \n",
            " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
            " |      - If float, then `min_samples_leaf` is a fraction and\n",
            " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            " |        number of samples for each node.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
            " |      The minimum weighted fraction of the sum total of weights (of all\n",
            " |      the input samples) required to be at a leaf node. Samples have\n",
            " |      equal weight when sample_weight is not provided.\n",
            " |  \n",
            " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
            " |      The number of features to consider when looking for the best split:\n",
            " |  \n",
            " |      - If int, then consider `max_features` features at each split.\n",
            " |      - If float, then `max_features` is a fraction and\n",
            " |        `int(max_features * n_features)` features are considered at each\n",
            " |        split.\n",
            " |      - If \"auto\", then `max_features=n_features`.\n",
            " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
            " |      - If None, then `max_features=n_features`.\n",
            " |  \n",
            " |      Note: the search for a split does not stop until at least one\n",
            " |      valid partition of the node samples is found, even if it requires to\n",
            " |      effectively inspect more than ``max_features`` features.\n",
            " |  \n",
            " |  max_leaf_nodes : int or None, optional (default=None)\n",
            " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            " |      Best nodes are defined as relative reduction in impurity.\n",
            " |      If None then unlimited number of leaf nodes.\n",
            " |  \n",
            " |  min_impurity_decrease : float, optional (default=0.)\n",
            " |      A node will be split if this split induces a decrease of the impurity\n",
            " |      greater than or equal to this value.\n",
            " |  \n",
            " |      The weighted impurity decrease equation is the following::\n",
            " |  \n",
            " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            " |                              - N_t_L / N_t * left_impurity)\n",
            " |  \n",
            " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
            " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
            " |  \n",
            " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            " |      if ``sample_weight`` is passed.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |  \n",
            " |  min_impurity_split : float, (default=1e-7)\n",
            " |      Threshold for early stopping in tree growth. A node will split\n",
            " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
            " |  \n",
            " |      .. deprecated:: 0.19\n",
            " |         ``min_impurity_split`` has been deprecated in favor of\n",
            " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
            " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
            " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
            " |  \n",
            " |  bootstrap : boolean, optional (default=True)\n",
            " |      Whether bootstrap samples are used when building trees. If False, the\n",
            " |      whole datset is used to build each tree.\n",
            " |  \n",
            " |  oob_score : bool, optional (default=False)\n",
            " |      whether to use out-of-bag samples to estimate\n",
            " |      the R^2 on unseen data.\n",
            " |  \n",
            " |  n_jobs : int or None, optional (default=None)\n",
            " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
            " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
            " |      <n_jobs>` for more details.\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, optional (default=None)\n",
            " |      Controls both the randomness of the bootstrapping of the samples used\n",
            " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
            " |      features to consider when looking for the best split at each node\n",
            " |      (if ``max_features < n_features``).\n",
            " |      See :term:`Glossary <random_state>` for details.\n",
            " |  \n",
            " |  verbose : int, optional (default=0)\n",
            " |      Controls the verbosity when fitting and predicting.\n",
            " |  \n",
            " |  warm_start : bool, optional (default=False)\n",
            " |      When set to ``True``, reuse the solution of the previous call to fit\n",
            " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
            " |      new forest. See :term:`the Glossary <warm_start>`.\n",
            " |  \n",
            " |  ccp_alpha : non-negative float, optional (default=0.0)\n",
            " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            " |      subtree with the largest cost complexity that is smaller than\n",
            " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  max_samples : int or float, default=None\n",
            " |      If bootstrap is True, the number of samples to draw from X\n",
            " |      to train each base estimator.\n",
            " |  \n",
            " |      - If None (default), then draw `X.shape[0]` samples.\n",
            " |      - If int, then draw `max_samples` samples.\n",
            " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
            " |        `max_samples` should be in the interval `(0, 1)`.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  base_estimator_ : DecisionTreeRegressor\n",
            " |      The child estimator template used to create the collection of fitted\n",
            " |      sub-estimators.\n",
            " |  \n",
            " |  estimators_ : list of DecisionTreeRegressor\n",
            " |      The collection of fitted sub-estimators.\n",
            " |  \n",
            " |  feature_importances_ : ndarray of shape (n_features,)\n",
            " |      The feature importances (the higher, the more important the feature).\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      The number of features when ``fit`` is performed.\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      The number of outputs when ``fit`` is performed.\n",
            " |  \n",
            " |  oob_score_ : float\n",
            " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
            " |      This attribute exists only when ``oob_score`` is True.\n",
            " |  \n",
            " |  oob_prediction_ : ndarray of shape (n_samples,)\n",
            " |      Prediction computed with out-of-bag estimate on the training set.\n",
            " |      This attribute exists only when ``oob_score`` is True.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
            " |  >>> from sklearn.datasets import make_regression\n",
            " |  \n",
            " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
            " |  ...                        random_state=0, shuffle=False)\n",
            " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
            " |  >>> regr.fit(X, y)\n",
            " |  RandomForestRegressor(max_depth=2, random_state=0)\n",
            " |  >>> print(regr.feature_importances_)\n",
            " |  [0.18146984 0.81473937 0.00145312 0.00233767]\n",
            " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
            " |  [-8.32987858]\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The default values for the parameters controlling the size of the trees\n",
            " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            " |  unpruned trees which can potentially be very large on some data sets. To\n",
            " |  reduce memory consumption, the complexity and size of the trees should be\n",
            " |  controlled by setting those parameter values.\n",
            " |  \n",
            " |  The features are always randomly permuted at each split. Therefore,\n",
            " |  the best found split may vary, even with the same training data,\n",
            " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
            " |  of the criterion is identical for several splits enumerated during the\n",
            " |  search of the best split. To obtain a deterministic behaviour during\n",
            " |  fitting, ``random_state`` has to be fixed.\n",
            " |  \n",
            " |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
            " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
            " |  [1], whereas the former was more recently justified empirically in [2].\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  \n",
            " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
            " |  \n",
            " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
            " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      RandomForestRegressor\n",
            " |      ForestRegressor\n",
            " |      sklearn.base.RegressorMixin\n",
            " |      BaseForest\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.ensemble._base.BaseEnsemble\n",
            " |      sklearn.base.MetaEstimatorMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from ForestRegressor:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict regression target for X.\n",
            " |      \n",
            " |      The predicted regression target of an input sample is computed as the\n",
            " |      mean predicted regression targets of the trees in the forest.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The predicted values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.RegressorMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the coefficient of determination R^2 of the prediction.\n",
            " |      \n",
            " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
            " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
            " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
            " |      The best possible score is 1.0 and it can be negative (because the\n",
            " |      model can be arbitrarily worse). A constant model that always\n",
            " |      predicts the expected value of y, disregarding the input features,\n",
            " |      would get a R^2 score of 0.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples. For some estimators this may be a\n",
            " |          precomputed kernel matrix or a list of generic objects instead,\n",
            " |          shape = (n_samples, n_samples_fitted),\n",
            " |          where n_samples_fitted is the number of\n",
            " |          samples used in the fitting for the estimator.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True values for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          R^2 of self.predict(X) wrt. y.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The R2 score used when calling ``score`` on a regressor will use\n",
            " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
            " |      with :func:`~sklearn.metrics.r2_score`. This will influence the\n",
            " |      ``score`` method of all the multioutput regressors (except for\n",
            " |      :class:`~sklearn.multioutput.MultiOutputRegressor`). To specify the\n",
            " |      default value manually and avoid the warning, please either call\n",
            " |      :func:`~sklearn.metrics.r2_score` directly or make a custom scorer with\n",
            " |      :func:`~sklearn.metrics.make_scorer` (the built-in scorer ``'r2'`` uses\n",
            " |      ``multioutput='uniform_average'``).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseForest:\n",
            " |  \n",
            " |  apply(self, X)\n",
            " |      Apply trees in the forest to X, return leaf indices.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
            " |          For each datapoint x in X and for each tree in the forest,\n",
            " |          return the index of the leaf x ends up in.\n",
            " |  \n",
            " |  decision_path(self, X)\n",
            " |      Return the decision path in the forest.\n",
            " |      \n",
            " |      .. versionadded:: 0.18\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
            " |          Return a node indicator matrix where non zero elements\n",
            " |          indicates that the samples goes through the nodes.\n",
            " |      \n",
            " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
            " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
            " |          gives the indicator value for the i-th estimator.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Build a forest of trees from the training set (X, y).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, its dtype will be converted\n",
            " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels in classification, real numbers in\n",
            " |          regression).\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. In the case of\n",
            " |          classification, splits are also ignored if they would result in any\n",
            " |          single class carrying a negative weight in either child node.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseForest:\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      Return the feature importances (the higher, the more important the\n",
            " |         feature).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : array, shape = [n_features]\n",
            " |          The values of this array sum to 1, unless all trees are single node\n",
            " |          trees consisting of only the root node, in which case it will be an\n",
            " |          array of zeros.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
            " |  \n",
            " |  __getitem__(self, index)\n",
            " |      Return the index'th estimator in the ensemble.\n",
            " |  \n",
            " |  __iter__(self)\n",
            " |      Return iterator over estimators in the ensemble.\n",
            " |  \n",
            " |  __len__(self)\n",
            " |      Return the number of estimators in the ensemble.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGQLrLeGmR7C",
        "outputId": "e6780a62-38b2-4f8d-ba6c-12109a7c5782",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Important features=', rfr.feature_importances_)\n",
        "sm = np.sum(rfr.feature_importances_)\n",
        "\n",
        "print('Sum=', sm)\n",
        "\n",
        "max_f = -np.sort(-rfr.feature_importances_)[0:2]\n",
        "\n",
        "max_f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Important features= [0.03211748 0.00154999 0.0070941  0.0011488  0.01436832 0.40270459\n",
            " 0.01424477 0.06403265 0.00496762 0.01169177 0.01808961 0.0123114\n",
            " 0.41567892]\n",
            "Sum= 0.9999999999999999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.41567892, 0.40270459])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtenL_ngATaI"
      },
      "source": [
        "#### *Задание 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWkIocYAAZV9"
      },
      "source": [
        "В этом задании мы будем работать с датасетом, с которым мы уже знакомы по домашнему заданию по библиотеке Matplotlib, это датасет Credit Card Fraud Detection.\n",
        "Для этого датасета мы будем решать задачу классификации - будем определять,какие из транзакциции по кредитной карте являются мошенническими.\n",
        "Данный датасет сильно несбалансирован (так как случаи мошенничества относительно редки),так что применение метрики accuracy не принесет пользы и не поможет выбрать лучшую модель.\n",
        "Мы будем вычислять AUC, то есть площадь под кривой ROC.\n",
        "Импортируйте из соответствующих модулей RandomForestClassifier, GridSearchCV и train_test_split.\n",
        "Загрузите датасет creditcard.csv и создайте датафрейм df.\n",
        "С помощью метода value_counts с аргументом normalize=True убедитесь в том, что выборка несбалансирована.Используя метод info, проверьте, все ли столбцы содержат числовые данные и нет ли в них пропусков.\n",
        "Примените следующую настройку, чтобы можно было просматривать все столбцы датафрейма:\n",
        "pd.options.display.max_columns = 100.\n",
        "Просмотрите первые 10 строк датафрейма df.\n",
        "Создайте датафрейм X из датафрейма df, исключив столбец Class.\n",
        "Создайте объект Series под названием y из столбца Class.\n",
        "Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split, используя аргументы: test_size=0.3, random_state=100, stratify=y.\n",
        "У вас должны получиться объекты X_train, X_test, y_train и y_test.\n",
        "Просмотрите информацию о их форме.\n",
        "Для поиска по сетке параметров задайте такие параметры:\n",
        "parameters = [{'n_estimators': [10, 15],\n",
        "'max_features': np.arange(3, 5),\n",
        "'max_depth': np.arange(4, 7)}]\n",
        "Создайте модель GridSearchCV со следующими аргументами:\n",
        "estimator=RandomForestClassifier(random_state=100),\n",
        "param_grid=parameters,\n",
        "scoring='roc_auc',\n",
        "cv=3.\n",
        "Обучите модель на тренировочном наборе данных (может занять несколько минут).\n",
        "Просмотрите параметры лучшей модели с помощью атрибута best_params_.\n",
        "Предскажите вероятности классов с помощью полученнной модели и метода predict_proba.\n",
        "Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и запишите в массив y_pred_proba. Из модуля sklearn.metrics импортируйте метрику roc_auc_score.\n",
        "Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных, используя в качестве аргументовмассивы y_test и y_pred_proba.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ptuf-4gBUgu",
        "outputId": "ae88759b-65c5-455b-a174-645163b58220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "df = pd.read_csv('//content/gdrive/My Drive/data/creditcard.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te-G8RUiDM4Q",
        "outputId": "996d836b-4727-4600-b644-3bfc91acc5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pd.options.display.max_columns = 100\n",
        "\n",
        "vc = df.value_counts(normalize=True)\n",
        "print(\"info:\\n\", df.info())\n",
        "print(\"Nulls:\\n\")\n",
        "df.isnull().astype(np.int).sum().astype(np.int)\n",
        "\n",
        "print(\"df[0-10]:\\n\", df[:10])\n",
        "\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "info:\n",
            " None\n",
            "Nulls:\n",
            "\n",
            "df[0-10]:\n",
            "    Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
            "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
            "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
            "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
            "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
            "\n",
            "         V8        V9       V10       V11       V12       V13       V14  \\\n",
            "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
            "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
            "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
            "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
            "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
            "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
            "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
            "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
            "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
            "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
            "\n",
            "        V15       V16       V17       V18       V19       V20       V21  \\\n",
            "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
            "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
            "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
            "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
            "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
            "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
            "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
            "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
            "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
            "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
            "\n",
            "        V22       V23       V24       V25       V26       V27       V28  \\\n",
            "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
            "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
            "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
            "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
            "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
            "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
            "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
            "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
            "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
            "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
            "\n",
            "   Amount  Class  \n",
            "0  149.62      0  \n",
            "1    2.69      0  \n",
            "2  378.66      0  \n",
            "3  123.50      0  \n",
            "4   69.99      0  \n",
            "5    3.67      0  \n",
            "6    4.99      0  \n",
            "7   40.80      0  \n",
            "8   93.20      0  \n",
            "9    3.68      0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibBdRChuEklg",
        "outputId": "01ffab46-a588-4005-ed6e-70af5abc89f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = df.drop(\"Class\", axis=1)\n",
        "y = df[\"Class\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y) \n",
        "#Просмотрите информацию о их форме.\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((199364, 30), (85443, 30), (199364,), (85443,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FQ1jLMVKtH5",
        "outputId": "bf732e47-25b4-46be-9c34-5418061206dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "parameters = [{'n_estimators': [10, 15],\n",
        "'max_features': np.arange(3, 5),\n",
        "'max_depth': np.arange(4, 7)}]\n",
        "#Создайте модель GridSearchCV со следующими аргументами:\n",
        "clf = GridSearchCV(\n",
        "  estimator=RandomForestClassifier(random_state=100), \n",
        "            param_grid=parameters, \n",
        "            scoring='roc_auc', \n",
        "            cv=3)\n",
        "# Обучите модель на тренировочном наборе данных (может занять несколько минут)\n",
        "clf.fit(X_train, y_train)\n",
        "clf.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 6, 'max_features': 3, 'n_estimators': 15}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxzi863NLwDJ",
        "outputId": "cda09bc0-aecf-4a45-f490-98f584507b1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Предскажите вероятности классов с помощью полученнной модели и метода predict_proba.\n",
        "pp = clf.predict_proba(X_test)\n",
        "# Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и запишите в массив y_pred_proba\n",
        "y_pred_proba = pp[:,1]\n",
        "y_pred_proba[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00092917, 0.00029521, 0.00028215, 0.00028215, 0.00028215])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTn-laexNIDy",
        "outputId": "1c9508f2-bcb1-497a-d979-840e576d7745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "#Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных, используя в качестве аргументов массивы y_test и y_pred_proba\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "auc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9462664156037156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}